<html>
<head>
    <title>Berkeley UPC installation sites</title>
</head>
<body bgcolor="#ffffff">

<!--#include virtual="/header.html" -->

<h1>Berkeley UPC Site and Job Startup Information</h1>

As a general rule, the UPC compiler presents a very portable interface to the
user across a wide variety of Operating Systems and High-Performance networks.
However, there are always site specific details such as programming
environments, compiler installations, batching systems that cannot be captured
by the build and runtime process.

For example, the GM conduit runs over Myrinet, a highly available network which
unfortunately presents no real job spawning interface.  The following table
describes the job startup mechanisms currently supported:

<p>
<h2>GASNet/GM Job Startup Mechanisms</h2>
<table border=1>
<th>Site Installation</th><th width="60%">Information</th><th>GASNet/GM spawning command</th>
<tr>
    <td>MPICH/gm</td>
    <td>Distributed as <tt>mpirun.ch_gm.pl</tt> with the MPICH distribution.
	it should be noted that GASNet/GM only supports the script from the
	MPICH 1.2.5..10 distribution</td>
    <td><tt>mpirun -np NP</tt></td>
<tr>
    <td>MPICH/gm or LAM</td>
    <td>The <tt>mpiexec</tt> utility provides the best portability and keeps
	track of Myricom's changes to the spawner interface (many sites have
	already adopted it and it is available from
	<a href="http://www.osc.edu/~pw/mpiexec/">http://www.osc.edu/~pw/mpiexec/</a>).
    </td>
    <td><tt>mpiexec -n NP</tt></td>
<tr>
    <td>gexec</td>
    <td>The <tt>gexec</tt> utility is another cluster remote execution system,
	and is often used in conjunction with the <tt>gstat</tt> and Ganglia
	cluster management software (available from
	<a href="http://www.theether.org/gexec/">http://www.theether.org/gexec/</a>).
	The <tt>GEXEC_SVRS</tt> can be set to target a specific list of nodes
	to be used for the job.  </td>
    <td><tt>gasnetrun_gm -n NP --gexec</tt></td>
<tr>
    <td>Others / Berkeley UPC Default</td>
    <td>As a fallback mechanism and also installed by default in
	<tt>uprun.conf</tt>, the GASNet/GM contrib directory contains a
	<tt>gasnetrun_gm.pl</tt> perl script.  It is added as a commodity for
	clusters which do not necessarily have some of the above site-specific
	spawners or for administrators which may want to tailor a spawning
	script. A <tt>'-v'</tt> verbose output may help debug the script.</td>
    <td><tt>gasnetrun_gm -n NP</tt></td>
</table>

<p>
<h2>Berkeley UPC installation sites</h2>
There are currently 4 different sites on which our UPC compiler is operating.

<h3>1. NERSC 'Alvarez' Myrinet Cluster</h3>

This is an experimental Myrinet/GM cluster at NERSC, which generally has rather
light traffic, yet is fairly stable.

<p>The login for the machine is <tt>alvarez.nersc.gov</tt>.  The homepage
(including information on how to get accounts) is at 
<pre>
    http://www.nersc.gov/alvarez/
</pre>

<p><b>To compile UPC programs on alvarez</b>

<ol>
    <li>Make sure that your shell startup files include a line to set up the
    'modules' system (they should have them unless you've overwritten the
    default .profile, etc. that were given to you as a new user).  For bash, use

    <p><tt>source /usr/local/pkg/Modules/init/bash</tt>

    <p><b>Note:</b> the alvarez convention seems to be for you to put any
    customizations in your <tt>.bashrc.ext</tt> file (or <tt>.profile.ext</tt>,
    <tt>.login.ext</tt>, etc.), rather than in <tt>.bashrc</tt> itself.

    <p><li> Make sure you have the 'upc', 'gcc', 'pbs', and 'maui' modules
    loaded, and make sure you do <b>not</b> have the 'pgi' one (which they load
    in by default in their initial login scripts) present.  You can do this
    either by editing the default module commands in your <tt>.bashrc</tt> file
    directly, or adding
    
    <p><tt>module delete pgi</tt><br> <tt>    module add upc gcc pbs
        maui</tt>

       <p>to your <tt>.bashrc.ext</tt> file.

    <p><li> In order to build the GM conduit, make sure the following two
    environment variables are set to point to the latest GM revision:
    <pre>
	GM_INCLUDE = /usr/gm/include
	GM_LIB     = /usr/gm/lib
    </pre>

    <p><li>Compile/link your program with <tt>upcc</tt> (see the <a
        href="upcc.html">upcc man page</a> for details).
    
</ol>

<p><b>To run UPC programs on alvarez</b>

<p>To run programs, you must use the PBS batch system.  Generally you'll want to do
this interactively:

<ol> <li>Enter PBS via 
    
        <p><tt>qsub -I -l nodes=2,walltime=5:00:00</tt>.  
        
        <p>This shouldn't take more than a minute or so (you can use
        <tt>qstat</tt> to examine the job queue), and you can remain in the
        session for up to 5 hours.  You will now be logged onto a compute node
        on the system, but the file system is largely the same, so you generally
        won't notice the difference.

	<p><li>The GM conduit install the <tt>gasnetrun_gm</tt> script in the
	GASNet bin directory and should be used to start jobs (as described in
	the GASNet/GM spawning mechanisms table above).  Alternatively, the
	mpirun script from MPICH 1.2.5..10 can be used exactly as described
	below for MPI jobs.

        <p><li>If you have built your UPC program to run over MPI instead of the
        default GM conduit (i.e., you compiled with "<tt>upcc -network mpi</tt>"),
        then you should <b>not</b> use the <tt>gasnetrun</tt> script, and
        instead should use the MPI startup script <tt>mpirun</tt> as follows:

        <p><tt>mpirun -machinefile $PBS_NODEFILE -np 2 executable_name</tt>

        <p>where '2' is the number of nodes, and 'executable_name' is your
        program.  To allow a little less typing, the upc module contains a
        program called <tt>mpirun-batch</tt>, which can simply be called as

        <p><tt>mpirun-batch executable_name</tt>


    <p><li>Exit the shell to leave PBS and return to your alvarez session.
</ol>


<h3>2. FTG PC Fast Ethernet Cluster</h3>

This is a system maintained by our group at LBL.  You'll need an account on
n2001.lbl.gov to use the cluster.

<p>The only conduit supported on the FTG cluster at present is MPI.

<p><b>To compile UPC programs on n2001</b>

<ol>

    <li>Make sure that your shell startup files include a line to set up the
    'modules' system (they should have them unless you've overwritten the
    default <tt>.profile</tt>, etc. that were given to you as a new user).  For
    bash, use

    <p><tt>source /usr/local/pkg/Modules/init/bash</tt>


    <p><li>Add 
    
    <p><tt>module add mpich pbs upc</tt> 
    
    <p>to your shell's startup script, to ensure you've got the correct MPI
    libraries, PBS, and our UPC compiler in your PATH.
    
    <p>Multiple versions of our UPC compiler are present on this system: use
    '<tt>modules avail</tt>' to see a list of them.  The version that a plain
    '<tt>module add upc</tt>' will load is shown with a '(default)'.  This is
    generally the right version to be using, but other versions can be loaded
    via '<tt>module load upc/1.0-debug</tt>', etc.  You can switch between
    versions via '<tt>module switch upc upc/stable-opt</tt>', for instance (you
    can always just use 'upc' as the first argument, no matter which upc module
    you actually currently have loaded).

    <p><li>Compile/link your program with <tt>upcc</tt> (see the <a
        href="upcc.html">upcc man page</a> for details).
    
</ol>

<p><b>To run UPC programs on n2001</b>

<p>To run programs, you must use the PBS batch system.  Generally you'll want to do
this interactively:

<ol>

    <li>Enter PBS via 
    
        <p><tt>qsub -I -l nodes=2,walltime=5:00:00</tt>.  
        
        <p>This shouldn't take more than a minute or so, and you can remain in
        the session for up to 5 hours.  You will now be logged onto a
        compute node on the system, but the file system is largely mapped to
        n2001's, so you generally won't notice the difference.

    <p><li>Run programs with <tt>'mpirun program'</tt>.  The number of nodes
    specified in your qsub command will automatically be used.

    <p><li>Exit the shell to leave PBS and return to your n2001 session.
</ol>

<p>More information on using the PC cluster is available at <a
    href="http://www.nersc.gov/research/FTG/pcp/user/x86cluster.html">http://www.nersc.gov/research/FTG/pcp/user/x86cluster.html</a>.


<h3>3. 'ymca' Linux Myrinet cluster</h3>

We have a temporary login to this system, located at ymca.lbl.gov.  It has a
particularly nice setup, being a very new machine, with Myrinet LANai9.2
and a 64bit/66MHz PCI bus.

<p> YMCA uses LAM/MPI instead of MPICH, which makes a difference for booting GM
jobs.  LAM uses a per-use daemon for process and environment control and the
'lamboot' utility must be run prior to invoking upcrun.  The connection to the
daemon is per-user and is thus remains active even if new terminals are created
to run more upc jobs.

<p>The 'lamboot' utility should only be run once -- any attempt to run
it more than once will kill all previous connections to the LAM
daemon.  The LAM/MPI environment on YMCA can be "booted" as follows:

<pre>
    wwnodes --mpi > lamhosts.out
    lamboot -v lamhosts.out

    upcrun -np 2 testprogram1
    upcrun -np 2 testprogram2
    upcrun -np 2 testprogram3
</pre>

If a different set of nodes is required, 'lamhalt' can be run and the
boot process can be run again.

<p>YMCA does not share the GM library across all slave nodes.  As a result, you
must always pass upcc '-Wl,-static' in order to compile the GM library into the
binary (note: this may change in the near future).  You can do this by putting
the flag in 'default_options' in your $HOME/.upccrc file.

<h3>4. Citris/Millennium UCB Cluster</h3>

This is a 32-node Myrinet/XP (GM-2) Itanium-2 cluster, where each node contains
4GB of memory.  The installation is rather brittle and NFS performance is
sometimes flaky.  

<p><b>To compile UPC programs on citrus</b>

<ol>
    <li> Because of the absence of modules, the <tt>PATH</tt> variable must be
    adjusted to contain
    <tt>/usr/mill/pkg/gm/bin:/usr/mill/pkg/mpich-gm/bin</tt>.  

    <li> The following additional environment variables must be set:
    <pre>
	CC         = /usr/bin/gcc-3.3
	GM_INCLUDE = /usr/gm/include
	GM_LIB     = /usr/gm/lib
    </pre>
</ol>

<p><b>To run UPC programs on citrus</b>
<p> The system currently does not contain a batch scheduler and there is no
node reservation mechanism other than gexec.  It is therefore deprecated for
performance runs, since stray processes can already be running on the selected
nodes.  Gexec either works in a mode where the user specifies the nodes to
use or resorts to using <tt>gstat</tt> which keeps some load balancing
information in order to select nodes to use.

<ol>
    <li> Specific nodes can be selected by setting the <tt>GEXEC_SVRS<tt>
         to a list of Internet hostname style:
    <pre>
	GEXEC_SVRS = "c17 c18 c19 c20"
    </pre>

    <li> Broken or unresponsive nodes can be filtered out by setting the
    <tt>GEXEC_SVRS_IGNORE</tt> variable to a list nodes to be ignored by gexec.
</ol>
Finally, a job can be started using
<pre>
    gasnetrun_gm -np NP --gexec
</pre>

<p>&nbsp;

<!-- don't touch stuff below this line -->
<hr>
<!--#include virtual="/footer.html"-->
</body>
</html>
